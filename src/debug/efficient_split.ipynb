{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.tts.utils_text import split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Few-shot object detection (FSOD) has received numerous attention due to the difficulty and time-consuming of labeling objects. Recent researches achieve excellent performance in a natural scene by only using a few instances of novel classes to fine-tune the last prediction layer of the model well-trained on plentiful base data. However, compared with natural scene objects with a single direction and small size variety, the direction and size of the objects in remote sensing images (RSIs) vary greatly. The methods proposed for the natural scene cannot be directly applied to RSIs. In this article, we first propose a strong baseline for RSIs. It fine-tunes all detector components acting on high-level features and effectively improves the performance of novel classes. Further analyzing the results of the baseline, we find that the error for novel classes is mainly concentrated in classification. It misclassifies novel classes as confusable base classes or backgrounds due to the difficulty in extracting generalized information from limited instances. As is well-known, text-modal knowledge can highly summarize the generalized and unique characteristics of categories. Thus, we introduce text-modal descriptions for each category and propose an FSOD method guided by TExt-MOdal knowledge, called TEMO. Specifically, a text-modal knowledge extractor and a cross-modal assembly module are proposed to extract text features and fuse the text-modal features into visual-modal feaxtract text features and fuse the text-modal features into visual-modal feaxtract text features and fuse the text-modal features into visual-modal feaxtract text features and fuse the text-modal features into visual-modal feaxtract text features and fuse the text-modal features into visual-modal features. The fused features greatly reduce the classification confusion of novel classes. Furthermore, we introduce a mask strategy and a separation loss to avoid over-fitting and ambiguity of text-modal features. Experimental results on detection in optical remote sensing images (DIOR), Northwestern Polytechnical University (NWPU), and fine-grained object recognition in high-resolution remote sensing imagery (FAIR1M) illustrate that our TEMO achieves state-of-the-art performance in all settings.\n",
    "\"\"\"\n",
    "\n",
    "rtf = 1.5\n",
    "min_length=10\n",
    "obj_length=200\n",
    "max_length=500\n",
    "split_pattern=r'\\.|\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125 Few-shot object detection (FSOD) has received numerous attention due to the difficulty and time-consuming of labeling objects\n",
      "201 Recent researches achieve excellent performance in a natural scene by only using a few instances of novel classes to fine-tune the last prediction layer of the model well-trained on plentiful base data\n",
      "175 However, compared with natural scene objects with a single direction and small size variety, the direction and size of the objects in remote sensing images (RSIs) vary greatly\n",
      "77 The methods proposed for the natural scene cannot be directly applied to RSIs\n",
      "60 In this article, we first propose a strong baseline for RSIs\n",
      "125 It fine-tunes all detector components acting on high-level features and effectively improves the performance of novel classes\n",
      "128 Further analyzing the results of the baseline, we find that the error for novel classes is mainly concentrated in classification\n",
      "155 It misclassifies novel classes as confusable base classes or backgrounds due to the difficulty in extracting generalized information from limited instances\n",
      "116 As is well-known, text-modal knowledge can highly summarize the generalized and unique characteristics of categories\n",
      "131 Thus, we introduce text-modal descriptions for each category and propose an FSOD method guided by TExt-MOdal knowledge, called TEMO\n",
      "478 Specifically, a text-modal knowledge extractor and a cross-modal assembly module are proposed to extract text features and fuse the text-modal features into visual-modal feaxtract text features and fuse the text-modal features into visual-modal feaxtract text features and fuse the text-modal features into visual-modal feaxtract text features and fuse the text-modal features into visual-modal feaxtract text features and fuse the text-modal features into visual-modal features\n",
      "79 The fused features greatly reduce the classification confusion of novel classes\n",
      "122 Furthermore, we introduce a mask strategy and a separation loss to avoid over-fitting and ambiguity of text-modal features\n",
      "286 Experimental results on detection in optical remote sensing images (DIOR), Northwestern Polytechnical University (NWPU), and fine-grained object recognition in high-resolution remote sensing imagery (FAIR1M) illustrate that our TEMO achieves state-of-the-art performance in all settings\n"
     ]
    }
   ],
   "source": [
    "# 1. Split by the pattern Trim whitespace and discard empty segments from the ends\n",
    "raw_segments = re.split(split_pattern, text)\n",
    "segments = [seg.strip() for seg in raw_segments if seg.strip()]\n",
    "\n",
    "for chunk in segments:\n",
    "    print(len(chunk), chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = = = = = = = = = = = = Number of words 17 Number of chars 125\n",
      "Split: ['Few-shot', 'object']\n",
      "current_char_count: 10 14\n",
      "Split: ['detection', '(FSOD)', 'has', 'received']\n",
      "current_char_count: 21.0 26\n",
      "Split: ['numerous', 'attention', 'due', 'to', 'the', 'difficulty', 'and', 'time-consuming']\n",
      "current_char_count: 39.0 52\n",
      "Split: ['of', 'labeling', 'objects']\n",
      "current_char_count: 78.0 17\n",
      " = = = = = = = = = = = = Number of words 32 Number of chars 201\n",
      "Split: ['Recent', 'researches', 'achieve', 'excellent', 'performance', 'in', 'a', 'natural', 'scene', 'by', 'only', 'using', 'a', 'few', 'instances']\n",
      "current_char_count: 78.0 82\n",
      "Split: ['of', 'novel', 'classes', 'to', 'fine-tune', 'the', 'last', 'prediction', 'layer', 'of', 'the', 'model', 'well-trained', 'on', 'plentiful', 'base', 'data']\n",
      "current_char_count: 123.0 88\n",
      " = = = = = = = = = = = = Number of words 28 Number of chars 175\n",
      "Split: ['However,', 'compared', 'with', 'natural', 'scene', 'objects', 'with', 'a', 'single', 'direction', 'and', 'small', 'size', 'variety,', 'the', 'direction', 'and', 'size', 'of', 'the', 'objects', 'in', 'remote', 'sensing', 'images', '(RSIs)']\n",
      "current_char_count: 132.0 137\n",
      "END OF WARMUP\n",
      "Split: ['vary', 'greatly']\n",
      "current_char_count: 200 11\n",
      " = = = = = = = = = = = = Number of words 13 Number of chars 77\n",
      "Split: ['The', 'methods', 'proposed', 'for', 'the', 'natural', 'scene', 'cannot', 'be', 'directly', 'applied', 'to', 'RSIs']\n",
      "current_char_count: 200 65\n",
      " = = = = = = = = = = = = Number of words 11 Number of chars 60\n",
      "Split: ['In', 'this', 'article,', 'we', 'first', 'propose', 'a', 'strong', 'baseline', 'for', 'RSIs']\n",
      "current_char_count: 200 50\n",
      " = = = = = = = = = = = = Number of words 17 Number of chars 125\n",
      "Split: ['It', 'fine-tunes', 'all', 'detector', 'components', 'acting', 'on', 'high-level', 'features', 'and', 'effectively', 'improves', 'the', 'performance', 'of', 'novel', 'classes']\n",
      "current_char_count: 200 109\n",
      " = = = = = = = = = = = = Number of words 20 Number of chars 128\n",
      "Split: ['Further', 'analyzing', 'the', 'results', 'of', 'the', 'baseline,', 'we', 'find', 'that', 'the', 'error', 'for', 'novel', 'classes', 'is', 'mainly', 'concentrated', 'in', 'classification']\n",
      "current_char_count: 200 109\n",
      " = = = = = = = = = = = = Number of words 21 Number of chars 155\n",
      "Split: ['It', 'misclassifies', 'novel', 'classes', 'as', 'confusable', 'base', 'classes', 'or', 'backgrounds', 'due', 'to', 'the', 'difficulty', 'in', 'extracting', 'generalized', 'information', 'from', 'limited', 'instances']\n",
      "current_char_count: 200 135\n",
      " = = = = = = = = = = = = Number of words 15 Number of chars 116\n",
      "Split: ['As', 'is', 'well-known,', 'text-modal', 'knowledge', 'can', 'highly', 'summarize', 'the', 'generalized', 'and', 'unique', 'characteristics', 'of', 'categories']\n",
      "current_char_count: 200 102\n",
      " = = = = = = = = = = = = Number of words 19 Number of chars 131\n",
      "Split: ['Thus,', 'we', 'introduce', 'text-modal', 'descriptions', 'for', 'each', 'category', 'and', 'propose', 'an', 'FSOD', 'method', 'guided', 'by', 'TExt-MOdal', 'knowledge,', 'called', 'TEMO']\n",
      "current_char_count: 200 113\n",
      " = = = = = = = = = = = = Number of words 64 Number of chars 478\n",
      "Split: ['Specifically,', 'a', 'text-modal', 'knowledge', 'extractor', 'and', 'a', 'cross-modal', 'assembly', 'module', 'are', 'proposed', 'to', 'extract', 'text', 'features', 'and', 'fuse', 'the', 'text-modal', 'features', 'into', 'visual-modal', 'feaxtract', 'text', 'features', 'and', 'fuse', 'the', 'text-modal', 'features', 'into']\n",
      "current_char_count: 200 200\n",
      "Split: ['visual-modal', 'feaxtract', 'text', 'features', 'and', 'fuse', 'the', 'text-modal', 'features', 'into', 'visual-modal', 'feaxtract', 'text', 'features', 'and', 'fuse', 'the', 'text-modal', 'features', 'into', 'visual-modal', 'feaxtract', 'text', 'features', 'and', 'fuse', 'the', 'text-modal', 'features', 'into', 'visual-modal']\n",
      "current_char_count: 200 207\n",
      "Split: ['features']\n",
      "current_char_count: 200 8\n",
      " = = = = = = = = = = = = Number of words 11 Number of chars 79\n",
      "Split: ['The', 'fused', 'features', 'greatly', 'reduce', 'the', 'classification', 'confusion', 'of', 'novel', 'classes']\n",
      "current_char_count: 200 69\n",
      " = = = = = = = = = = = = Number of words 18 Number of chars 122\n",
      "Split: ['Furthermore,', 'we', 'introduce', 'a', 'mask', 'strategy', 'and', 'a', 'separation', 'loss', 'to', 'avoid', 'over-fitting', 'and', 'ambiguity', 'of', 'text-modal', 'features']\n",
      "current_char_count: 200 105\n",
      " = = = = = = = = = = = = Number of words 34 Number of chars 286\n",
      "Split: ['Experimental', 'results', 'on', 'detection', 'in', 'optical', 'remote', 'sensing', 'images', '(DIOR),', 'Northwestern', 'Polytechnical', 'University', '(NWPU),', 'and', 'fine-grained', 'object', 'recognition', 'in', 'high-resolution', 'remote', 'sensing', 'imagery', '(FAIR1M)', 'illustrate', 'that', 'our']\n",
      "current_char_count: 200 201\n",
      "Split: ['TEMO', 'achieves', 'state-of-the-art', 'performance', 'in', 'all', 'settings']\n",
      "current_char_count: 200 52\n"
     ]
    }
   ],
   "source": [
    "current_char_count = 0\n",
    "current_char_objectitive = min_length\n",
    "maximum_char_objectitive = min_length\n",
    "current_chunk = []\n",
    "\n",
    "\n",
    "warmup = True\n",
    "\n",
    "final_chunk_list = []\n",
    "\n",
    "for segment in segments:\n",
    "    words = re.split(r'\\s+', segment)\n",
    "    number_words = len(words)\n",
    "\n",
    "    print(' = = = = = = = = = = = = Number of words',number_words, 'Number of chars', len(segment))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "\n",
    "        current_char_count += len(word)\n",
    "        current_chunk += [word]\n",
    "\n",
    "        \n",
    "        if current_char_count >= current_char_objectitive or i+1 == number_words:\n",
    "            \n",
    "            print('Split:', current_chunk)\n",
    "            print('current_char_count:', current_char_objectitive,current_char_count)\n",
    "            if current_char_count * rtf > obj_length and warmup:\n",
    "                warmup = False\n",
    "                print('END OF WARMUP')\n",
    "\n",
    "\n",
    "            if warmup:\n",
    "\n",
    "                maximum_char_objectitive = max(maximum_char_objectitive, current_char_count)\n",
    "                current_char_objectitive = maximum_char_objectitive * rtf            \n",
    "                final_chunk_list.append(' '.join(current_chunk))\n",
    "\n",
    "                current_char_count = 0\n",
    "                current_chunk = []\n",
    "            \n",
    "            else:\n",
    "                current_char_objectitive = obj_length     \n",
    "                final_chunk_list.append(' '.join(current_chunk))\n",
    "\n",
    "                current_char_count = 0\n",
    "                current_chunk = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 Few-shot object\n",
      "29 detection (FSOD) has received\n",
      "59 numerous attention due to the difficulty and time-consuming\n",
      "19 of labeling objects\n",
      "96 Recent researches achieve excellent performance in a natural scene by only using a few instances\n",
      "104 of novel classes to fine-tune the last prediction layer of the model well-trained on plentiful base data\n",
      "162 However, compared with natural scene objects with a single direction and small size variety, the direction and size of the objects in remote sensing images (RSIs)\n",
      "12 vary greatly\n",
      "77 The methods proposed for the natural scene cannot be directly applied to RSIs\n",
      "60 In this article, we first propose a strong baseline for RSIs\n",
      "125 It fine-tunes all detector components acting on high-level features and effectively improves the performance of novel classes\n",
      "128 Further analyzing the results of the baseline, we find that the error for novel classes is mainly concentrated in classification\n",
      "155 It misclassifies novel classes as confusable base classes or backgrounds due to the difficulty in extracting generalized information from limited instances\n",
      "116 As is well-known, text-modal knowledge can highly summarize the generalized and unique characteristics of categories\n",
      "131 Thus, we introduce text-modal descriptions for each category and propose an FSOD method guided by TExt-MOdal knowledge, called TEMO\n",
      "178 Specifically, a text-modal knowledge extractor and a cross-modal assembly module are proposed to extract text features and fuse the text-modal features into visual-modal features\n",
      "79 The fused features greatly reduce the classification confusion of novel classes\n",
      "122 Furthermore, we introduce a mask strategy and a separation loss to avoid over-fitting and ambiguity of text-modal features\n",
      "227 Experimental results on detection in optical remote sensing images (DIOR), Northwestern Polytechnical University (NWPU), and fine-grained object recognition in high-resolution remote sensing imagery (FAIR1M) illustrate that our\n",
      "58 TEMO achieves state-of-the-art performance in all settings\n"
     ]
    }
   ],
   "source": [
    "for chunk in final_chunk_list:\n",
    "    print(len(chunk), chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
